# What is t-SNE ? 
t-SNE ( t-Distributed Stochastic Neighbor Embedding) is a technique that visualizes high dimensional data by giving each point a location in a two or three dimensional map. The technique is the variation of Stochastic Neighbor Embedding (SNE) that is much easier to optimize and produces significantly better visualization.

There can be several other techniques which can be used for visualizing high dimensional data, like: PCA, which is a linear technique that focuses on keeping the low dimensional representation of dissimilar data points far apart. For a high dimensional data that lies on or near a low dimensional nonlinear manifold, it is usually more important to keep the low dimensional representation of very similar data points as close as possible, which is typically not possible with the linear mapping. t-SNE is capable of capturing much of the local structure of the high dimensional data, while also revealing the global structure, such as, presence of cluster at several scales. Figure 1 shows use of t-SNE on MNIST dataset which consist of images of hand written digits.

![](/images/Fig1_tsne.png "Source : Visualizing Data using t-SNE, research paper by Laurens van der Maaten and Geoffrey Hinton")

It starts by converting the high dimensional Euclidean distances between data points into joint probabilities that represent similarities. For example, Similarity of high dimensional data point x<sub>j</sub> to that of x<sub>i</sub> is the joint probability p<sub>ji</sub>. **p<sub>ji</sub> can be interpreted as probability that xi would pick xj as its neighbor, if neighbor were picked in proportion to their probability density under Normal(Gaussian) distribution**. For nearby point p<sub>ji</sub> will be high whereas, for widely separated data points p<sub>ji</sub> will be infinitesimal. Figure 2 represents how distances are mapped as probabilities on a Normal curve.

![](/images/fig2_tsne.png "Source: ’ StatQuest: t-SNE, Clearly Explained’ Youtube ") 

Now for the low dimensional counterpart say, y<sub>i</sub> and y<sub>j</sub> of the high dimensional data points, x<sub>i</sub> and x<sub>j</sub>, it is possible to compute similar joint probability q<sub>ji</sub>. Since, in high dimensional space we converted distances into probabilities using a Normal distribution, in the low dimensional map we can use a probability distribution that has a much heavier tail than Normal, to convert distances into probabilities. This allows a moderate distance in the high dimensional space to be faithfully modeled by a much larger distance in the map, and as a result it eliminates the unwanted attractive forces between map points, which represents moderately dissimilar data points **(Crowding Problem). In t-SNE we employ a student t-distribution** as a heavy tailed distribution in the low dimensional map. Figure 3 illustrates the differences between Normal distribution and t-distribution.

![](/images/fig3_tsne.png "Source: ’ StatQuest: t-SNE, Clearly Explained’ Youtube ") 

If the map points y<sub>i</sub> and y<sub>j</sub> correctly model the similarity between high dimensional data points x<sub>i</sub> and x</sub>j</sub>, the joint probabilities p<sub>ji</sub>= q<sub>ji</sub>. Therefore, **t-SNE aims to find a low dimensional representation that minimizes the mismatch between p<sub>ij</sub> and q<sub>ji</sub>**.

In computation of t-SNE there is a parameter involved called **‘perplexity’** which can be interpreted as a **smooth measure of effective number of neighbors**, whose typical value is between 5 and 50.

**Reference**: Visualizing Data using t-SNE, research paper by Laurens van der Maaten and Geoffrey Hinton.
