# What Is K-Means Clustering?
When it is your first day at school, you meet people you barely know and after spending days, you become friends with some of them, based on similarities. Clustering is exactly this.

**It refers to a very broad set of techniques for finding subgroups, or clusters in a data set**. When we cluster the observations of a dataset we seek to partition them into distinct groups so that the **observations within each group are quite similar to each other**, while observations in different groups are quite different. An application of clustering arises in marketing. We may have access to a large number of measurements (example:median household income, occupation, distance from the nearest urban area, and so forth) for a large number of people. Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product. The task of performing market segmentation amounts to clustering the people in the data set.

In general, we can cluster observations on the basis of the features in order to identify subgroups among the observations, or we can cluster features on the basis of the observations in order to discover subgroups among the features.

Since clustering is popular in many ﬁelds, there exist a great number of clustering methods. But, we will be specifically focusing on K-means clustering, where we seek to partition the observation into **pre-specified number of clusters**.

K-means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters. By non-overlapping we mean that no observation belongs to more than one cluster. To perform K-means clustering, we must ﬁrst specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the K clusters. Figure.1. shows the results obtained from performing K-means clustering on a simulated example consisting of 150 observations in two dimensions, using three diﬀerent values of K.

![](/images/k_means_1.JPG "Source: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.")

The idea behind K-means clustering is that a good clustering is one for which the **within-cluster variation is as small as possible**. That is, the difference between the observations in the same cluster is as small as possible. Here the question arises: *How to define within-cluster variation?*

There are many possible ways to deﬁne this concept, but by far the most common choice involves **squared Euclidean distance, where we compute the sum of square of differences amongst the observations with in the cluster**, which gives us magnitude of how close or how far the observations are from each other, now since our goal is to minimize this within cluster variation, this is in fact a very diﬃcult problem to solve precisely, since there are almost K to the power n ways to partition n observations into K clusters. This is a huge number unless K and n are tiny! Fortunately, a very simple algorithm can be shown to provide a local optimum.
_______________________________________________________________________

Algorithm: K-Means Clustering :
            
1. Randomly assign a number, from 1 to K, to each of the observations. These serve as initial cluster assignments for the observations.
            
2. Iterate (repeat) until the cluster assignments stop changing:

    a. For each of the K clusters, compute the cluster centroid. The kth cluster centroid is the vector of the p feature means (average) for the observations in the kth cluster.

    b. Assign each observation to the cluster whose centroid is closest (where closest is deﬁned using Euclidean distance).
    
________________________________________________________________________        
In step 2(a) we compute the mean of the observations within the cluster (cluster centroid). Here an observation is nothing but a vector of p features. Then in step 2(b) we compute distance between the observation to every cluster centroid and then assign the observation to the cluster for which the distance is minimum. This means that as the algorithm is run, the clustering obtained will continually improve until the result no longer changes. When the result no longer changes, a local optimum has been reached. Figure 2 shows the progression of the algorithm on the toy example from Figure 1.

![](/images/k_means_2.JPG "Source: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.")

K-means clustering derives its name from the fact that in Step 2(a), the cluster centroids are computed as the mean of the observations assigned to each cluster. Because the K-means algorithm ﬁnds a local optimum (minimizing the within cluster variance) rather than a global, the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1 of Algorithm. For this reason, it is important to run the algorithm multiple times from diﬀerent random initial conﬁgurations. Then one selects the best solution, that is, **Sum of within cluster variances of all the clusters (objective) is smallest**. Figure 3 shows the local optima obtained by running K-means clustering six times using six diﬀerent initial cluster assignments, using the toy data from Figure 1. In this case, the best clustering is the one with an objective value of 235.8(least).

![](/images/k_means_3.JPG "Source: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.")

As we have seen, to perform K-means clustering, we must decide how many clusters we expect in the data. The problem of selecting K is far from simple, and we may not know in advance how many clusters to select. Therefore, it is one of the drawbacks of K-means clustering. The mathematics of how clustering is performed is too technical to be discussed here.

**Reference**: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.
